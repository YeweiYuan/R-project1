---
title: "a brief analysis project"
output:
  html_document: default
  word_document: default
  pkdd_document: default
---

Here we'll try SVM and Random Forest. 
### 1.Load up the data

Continue with the KDD dataset from the previous workshop. We'll read it in as before.

```{r}
kdd<-read.csv("kddcup.data_10_percent.gz")
kddnames=read.table("kddcup.names",sep=":",skip=1,as.is=T)
colnames(kdd)=c(kddnames[,1],"normal")
sum(is.na(kdd)) #check
```

### 2.Load up needed packages

Before we start, we'll load some nessesary packages.

```{r}
library(tidyverse) # Data Wrangling and Transformation
library(caret) # Data preprocessing and Analysis models
library(dplyr) #filter
library(party)
library(randomForest) #randomForest
library(e1071) #svm
```

### 3.Data preprocessing
3.1 try SVD for dimension reduction
```{r}
a=sapply(kdd[5:41],sum,1)
print(a)
```

```{r}
kddsvd <- kdd[,-c(20,21)]
```

```{r}
#Create Dummy Variables
facots <- names(kddsvd)[sapply(kddsvd, class) == 'factor'] 
#Convert factor variable to the right half of formula
formulasvd <- as.formula(paste('~', paste(facots, collapse = '+')))
dummysvd <- dummyVars(formula = formulasvd, data = kddsvd)
predsvd <- data.frame(predict(dummysvd, newdata = kddsvd))
kdd_svd <- cbind(kddsvd[,1],predsvd)
kddsvd <- cbind(kddsvd[,5:39],kdd_svd)
```

```{r}
kddsvd <- kddsvd[,-c(118)]
kddsvd1 <- kddsvd[,-c(117)]
kddsvd1=scale(kddsvd1, center=T,scale=T)
kddsvd1=data.frame(kddsvd1)
kddsvd1.svd <- svd(kddsvd1)
```

```{r}
plot(-kddsvd1.svd$v[,1],
     -kddsvd1.svd$v[,2],xlab="-PC1",ylab="-PC2",main="svd(X)$v",
     col=as.numeric(kddsvd[,117]+2), pch=19,cex=0.5)

plot(kddsvd1.svd$d^2/sum(kddsvd1.svd$d^2), xlim = c(0,100), type = "b", pch = 16, xlab = "principal components", 
    ylab = "variance explained")
abline(v=c(7,20,88),col="red")

plot(kddsvd1.svd$d^2/sum(kddsvd1.svd$d^2), xlim = c(0,20), type = "b", pch = 16, xlab = "principal components", 
    ylab = "variance explained")
abline(v=c(4),col="red")
```

```{r}
d=NULL
d[0]=0
for (i in 1:70) {
  d[i]=sum(kddsvd1.svd$d[1:i]^2)/sum(kddsvd1.svd$d^2)
}
d
```

It is not a good way to use SVD

3.2 Deletion of zero variance variables (common part)
```{r}
rm_col <- nzv(kdd) #default percent
print(rm_col)
kdd_p1 <- kdd[,-rm_col]
```

We can see the variable "protocol_type" also be removed, after I checked it, I found there are more than 99% values of it is zero. Therefore, I needn't to transform the big number using log.

3.3 Deletion of relevant variables (common part)
```{r}
#Return the upper triangle value of correlation coefficient matrix
corr <- cor(kdd_p1[,4:18]) 
corr[upper.tri(corr)]
```

We can find that there are many relevant variables,

Show the variables whose corr>0.8 and remove them
```{r}
rm_col2 = findCorrelation(corr, cutoff = .9) + 3 #I didn't use the first three colume when colculate the corr.
print(rm_col2) 
kdd_p2 <- kdd_p1[,-rm_col2]
```

3.4 Deletion of noise(in the col4 and col6) (common part)
```{r}
boxplot.stats(kdd_p2[,6],)$out 
noise=boxplot.stats(kdd_p2[,4],coef=10)$out
length(noise)
kdd_p3=filter(kdd_p2,!src_bytes %in% c(noise))
```

```{r}
table(kdd$normal)
table(kdd_p3$normal)
```
```{r}
boxplot.stats(kdd_p2[,6],)$out 
noise=boxplot.stats(kdd_p2[,4],coef=60)$out#change the function to remain most of data and nomalized them(only remove some greater than 100000)
length(noise)
kdd_p3=filter(kdd_p2,!src_bytes %in% c(noise))
```

3.5 Factor value preprocessing (the last common part)
```{r}
table(kdd_p3$protocol_type)
table(kdd_p3$service)
table(kdd_p3$flag)
```

We need to simplify "service" and "flag"

```{r}
kdd_p4<-kdd_p3
kdd_p4 <- kdd_p4 %>% 
  mutate(service = as.character(service)) %>%
  mutate(flag = as.character(flag))
for (x in names(sort(table(kdd_p4$service))[1:59])) {
  kdd_p4$service[kdd_p4$service == x]<-"OTHER_"
}
for (x in c("OTH","RSTOS0","RSTR","S1","S2","S3","SH")) {
  kdd_p4$flag[kdd_p4$flag == x]<-"OTHER"
}
kdd_p4 <- kdd_p4 %>% 
  mutate(service = as.factor(service)) %>%
  mutate(flag = as.factor(flag))
```

```{r}
table(kdd_p4$protocol_type)
table(kdd_p4$service)
table(kdd_p4$flag)
```


3.6 Preprocess "normal" type
```{r}
table(kdd_p4$normal)
```

3.6.1 Ignore some types
```{r}
othernomal1 <- function(data){
  data <- data %>% 
  mutate(normal = as.character(normal)) 
  for (x in c("buffer_overflow.","ftp_write.","guess_passwd.","imap.","land.","loadmodule.","multihop.","perl.","nmap.","phf.","pod.","rootkit.","spy.","warezmaster.")) {
  data$normal[data$normal == x]<-"OTHER."
  }
  data <- data %>% 
  mutate(normal = as.factor(normal)) 
}
kdd_types <- othernomal1(kdd_p4)
table(kdd_types$normal)
```

3.6.2 Ignore all the nonnormal types
```{r}
othernomal2<-function(data){
  data <- data %>% 
  mutate(normal = as.character(normal)) 
  data$normal[data$normal!= "normal."]<-"OTHER."
  data <- data %>% 
  mutate(normal = as.factor(normal)) 
}
kdd_bin<-othernomal2(kdd_p4)
table(kdd_bin$normal)
```

3.7 Create Dummy Variables(for logist and SVM, RF will auto deal)
```{r}
kdd_bin_DV <- kdd_bin
# Filter out all factor variables
facots <- names(kdd_bin_DV)[sapply(kdd_bin_DV, class) == 'factor'] 
#Convert factor variable to the right half of formula
formula <- as.formula(paste('~', paste(facots, collapse = '+')))
dummy <- dummyVars(formula = formula, data = kdd_bin_DV)
pred <- data.frame(predict(dummy, newdata = kdd_bin_DV))
kdd_bin_DV <- cbind(kdd_bin_DV[,4:11],pred)
#去掉线性相关的，原来每个因子变量各删除一列
kdd_bin_DV <- kdd_bin_DV[,-c(9 ,12 ,24 ,26)]
```

3.8 Standardize(only for SVM)

#data for SVM can't be on totally different scales, so scale the input data

```{r}
kdd_bin_DVS <- kdd_bin_DV
kdd_bin_DVS[1:21]=scale(kdd_bin_DVS[1:21], center=T,scale=T) 
```

### 4.Training with SVM and Random Forest(Classification Algorithm)
4.1 Creat Traindata  
```{r}
Used_rows <- c(0) # record the used data
Traindata<-function(data,p=0.10,flag=2,value=names(table(data$normal))){
  if(flag == 1){
  data <- data %>% 
  mutate(normal = as.character(normal)) %>% 
  filter(normal %in% value) %>% 
  mutate(normal = factor(normal))
  }
  set.seed(1) # make the split repeatable
train_rows <- sample(1:nrow(data), size = p*nrow(data))
Traindata <- data[train_rows, ]
Used_rows <<- union(Used_rows,train_rows) 
return(Traindata)
}
```

#data is the original data,

#flag is 1 for don't ignore the type of nonormal traffic,2 means ignor it,

#value is the type user want analysis.

4.2 Train both a Random Forest and a Support Vector Machine.

Traing the first model using Random Forest, and the second for a Support Vector Machine.

We can compare the two and see which performs better on the data.

4.2.1 RandomForest

only consider normal or not:
```{r}
# Create the forest.
rfFit <- randomForest(normal ~ protocol_type + service	+ flag + src_bytes + logged_in + count + serror_rate + diff_srv_rate + dst_host_diff_srv_rate + dst_host_same_src_port_rate + dst_host_srv_rerror_rate,
                      data = Traindata(kdd_bin,p=0.1))
# View the forest results.
print(rfFit) 
```

consider the Types:
```{r}
# Create the forest.
rfFit2 <- randomForest(normal ~ protocol_type + service	+ flag + src_bytes + logged_in + count + serror_rate + diff_srv_rate + dst_host_diff_srv_rate + dst_host_same_src_port_rate + dst_host_srv_rerror_rate,
           data = Traindata(kdd_types,flag= 1))
# View the forest results.
print(rfFit2) 
```

only consider three main Types:
```{r}
# Create the forest.
rfFit3 <- randomForest(normal ~ protocol_type + service	+ flag + src_bytes + logged_in + count + serror_rate + diff_srv_rate + dst_host_diff_srv_rate + dst_host_same_src_port_rate + dst_host_srv_rerror_rate,
           data = Traindata(kdd_types,flag= 1,value = c("normal.","neptune.","smurf.")))
# View the forest results.
print(rfFit3) 
```

4.2.2 Look at the variable importance in the Random Forest Model

```{R}
print(importance(rfFit,type = 2))
opar = par(mfrow=c(1,2))
plot(varImp(rfFit))
title('without Types')
print(importance(rfFit2,type = 2))
plot(varImp(rfFit2))
title('with Types')
par(opar)
```

we can see that there's totally different importance between the condition whether analysis the type of the nonormal or not.

#should we remove some variables that don't revalent? Is that make sense?

4.2.3 SVM

find a suitable modle
```{r}
svm_test <- function(x,y){
  type <- 'C-classification'
  kernel <- c('linear','polynomial','radial','sigmoid')
  
  errors <- c(0,0,0,0)
  for(j in 1:4){
      pred <- predict(object = svm(x, y, type = 'C-classification', kernel = kernel[j]), newdata = x)
      errors[j] <- sum(as.integer(pred) != as.integer(as.factor(y)))
  }
  i= which.min(errors)
 }
```

```{r}
Tr <- Traindata(kdd_bin_DVS,p=0.01)
print(svm_test(Tr[,1:21],Tr[,22]))
```
```{r}
tune.svm(normal.normal. ~., data =Tr, gamma = seq(from=0.01, to=0.1, by=0.02), cost = seq(from=100, to=1000, by=100))
```

```{r}
Tr <- Traindata(kdd_bin_DVS,p=0.1)
weights <- c(1,4)
names(weights) <- c('1','0')
svmFit <- svm(Tr[,1:21], Tr[,22], type = 'C-classification', kernel = 'polynomial',gamma = 0.07,cost=500, class.weights = weights)
pred <- predict(svmFit, newdata = Tr[,1:21])
Freq <- table(Tr[,22], pred)
Freq
accuracy <- sum(diag(Freq))/sum(Freq)
accuracy
```


### 5. Logistic(Regression Algorithm)
```{r}
glm <- glm(normal.normal. ~ src_bytes + logged_in + count + serror_rate + diff_srv_rate + dst_host_diff_srv_rate+ dst_host_same_src_port_rate + dst_host_srv_rerror_rate + protocol_type.tcp + protocol_type.udp + service.ecr_i + service.ftp_data + service.http + service.other + service.OTHER_ + service.private + service.smtp + flag.OTHER + flag.REJ + flag.RSTO + flag.S0,
           family = binomial, data = Traindata(kdd_bin_DV))

summary(glm)
```

### 6. predata Transform functions (solve some prob. to predict, so that data from other dataset can be used)
6.1 Common Part
```{r}
Tran0 <- function(data){
  data <- data[,-rm_col]
  data <- data[,-rm_col2]
  
  data <- data %>% 
  mutate(service = as.character(service)) %>%
  mutate(flag = as.character(flag))
for (x in names(sort(table(data$service))[1:59])) {
  data$service[data$service == x]<-"OTHER_"
}
for (x in c("OTH","RSTOS0","RSTR","S1","S2","S3","SH")) {
  data$flag[data$flag == x]<-"OTHER"
}
data <- data %>% 
  mutate(service = as.factor(service)) %>%
  mutate(flag = as.factor(flag))
}
```

6.2 For Random Forest
```{r}
Tran1 <- function(data, flag= 2){
  data <- Tran0(data)
  if(flag == 1){
    data<-othernomal1(data)
  }
  if(flag == 2){
    data<-othernomal2(data)
  }
}
```

when flag=1 can use to predict Types

6.3 For SVM
```{r}
Tran2 <- function(data){
  data <- Tran0(data)
  data <- othernomal2(data)
  
  facots <- names(data)[sapply(data, class) == 'factor'] 
  formula <- as.formula(paste('~', paste(facots, collapse = '+')))
  dummy <- dummyVars(formula = formula, data = data)
  pred <- data.frame(predict(dummy, newdata = data))
  data <- cbind(data[,4:11],pred)
  data <- data[,-c(9 ,12 ,24 ,26)]
  data[1:21]=scale(data[1:21], center=T,scale=T)
  return(data)
}
```

6.4 For Logistic
```{r}
Tran3 <- function(data){
  data <- Tran0(data)
  data <- othernomal2(data)
  
  facots <- names(data)[sapply(data, class) == 'factor'] 
  formula <- as.formula(paste('~', paste(facots, collapse = '+')))
  dummy <- dummyVars(formula = formula, data = data)
  pred <- data.frame(predict(dummy, newdata = data))
  data <- cbind(data[,4:11],pred)
  data <- data[,-c(9 ,12 ,24 ,26)]
  return(data.frame(data))
}
```

### 7.Metric

7.1 TestData

7.1.1
```{R}
RemainTestdata <- kdd[-Used_rows, ]
Test_data <- function(data,p=0.50,flag=2,value=names(table(data$normal))){
  if(flag == 1){
  data <- data %>% 
  mutate(normal = as.character(normal)) %>% 
  filter(normal %in% value) %>% 
  mutate(normal = factor(normal))
  }
  set.seed(1) # make the split repeatable
test_rows <- sample(1:nrow(data), size = p*nrow(data))
Testdata <- data[test_rows, ]
return(Testdata)
}
```

7.1.2 RandomForest
```{R}
Testdatar <- Test_data(Tran1(RemainTestdata),p=0.5)
prer <- predict(rfFit, Testdatar)
```

```{r}
Testdatar <- Test_data(Tran1(RemainTestdata),p=0.5)
prer_2 <- predict(rfFit2, Testdatar)
levels(prer_2)
levels(prer_2)[c(1:3,5:10)]="OTHER."
prer_2 <- as.character(prer_2)
prer_2 <- as.factor(prer_2)
```

```{r}
Testdatar <- Test_data(Tran1(RemainTestdata),p=0.5)
prer_3 <- predict(rfFit3, Testdatar)
levels(prer_3)
```

```{r}
levels(prer_3)[c(1,3)]="OTHER."
prer_3 <- as.character(prer_3)
prer_3 <- as.factor(prer_3)
```

7.1.3 SVM
```{R}
Testdatas <- Test_data(Tran2(RemainTestdata),p=0.5)
pres <- predict(svmFit, Testdatas[,1:21])
```

7.1.4 Logistic
```{R}
Testdatal <- Test_data(Tran3(RemainTestdata),p=0.5)
prel <- as.factor(ifelse(predict(glm, Testdatal)>0.5,1,0))
```

7.2 Confusion matrix 

Look at the confusion matrix on the Random Forst model using the test data 

```{R}
print(confusionMatrix(prer,Testdatar$normal))
print(confusionMatrix(prer_2,Testdatar$normal))
print(confusionMatrix(prer_3,Testdatar$normal))
```

Look at the confusion matrix on the SVM model using the test data 

```{R}
print(confusionMatrix(pres,as.factor(Testdatas$normal.normal.)))
```

Look at the confusion matrix on the Logistic model using the test data 
```{R}
print(confusionMatrix(prel,as.factor(Testdatal$normal.normal.)))
```

the result of RandomForest looks better.

7.3 Plot ROC curves
```{r}
library(pROC)
```

```{r}
roc1 <- roc(Testdatar$normal,as.numeric(prer))
roc2 <- roc(Testdatas$normal.normal.,as.numeric(pres))
roc3 <- roc(Testdatal$normal.normal.,as.numeric(prel))
plot(roc1, col="green")  
plot.roc(roc2, add=TRUE, col="blue")
plot.roc(roc3, add=TRUE, col="red")
```
